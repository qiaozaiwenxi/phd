\documentclass[11pt,a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage[T1]{fontenc}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage[breaklinks=true,hidelinks]{hyperref}
% \usepackage[breaklinks=true]{hyperref}
% \usepackage[xindy, toc, nonumberlist]{glossaries}
\usepackage[toc, acronym, nonumberlist]{glossaries}
% \gls{label} to invoke the entry.
% \glsreset{label} to reset the first occurence of the entry.
% \acrlong{label} to print the long version of the acronym.
% \acrshort{label} to print the short version of the acronym.

\usepackage{todonotes}

\usepackage{etoolbox}
\AtBeginEnvironment{quote}{\itshape} 	% https://tex.stackexchange.com/questions/235852/how-to-make-all-quotes-italicised
										% 

\usepackage{fullpage}
% \usepackage{palatino}
\usepackage{tgtermes}
% \renewcommand{\familydefault}{\sfdefault}


% \usepackage[]{natbib}
\usepackage[numbers]{natbib}
\usepackage{amsthm}%theorems
\newtheoremstyle{customdef}%
	{\topsep}% above the theorem
	{\topsep}% below the theorem
	{\itshape}% body
	{0pt}% indent
	{\bfseries}% head
	{\newline}% punctuation between head and body
	{ }% Space after theorem head
	{\thmname{#1}\thmnumber{ #2}: \thmnote{#3}}
\theoremstyle{customdef}
\newtheorem{definition}{Definition}

%https://tex.stackexchange.com/questions/84478/how-to-format-title-and-abstract
\renewenvironment{abstract}{%
\begin{center}\begin{minipage}{0.85\textwidth}
\rule{\textwidth}{1pt}}
{\par\noindent\rule{\textwidth}{1pt}\end{minipage}\end{center}}


\makeglossaries
\include{glossary}

\author{Quentin Delhaye}
\date{September 11th, 2017}
\title{Automated System Partitioning for Efficient 3D Circuit Integration \\ \vspace{.5em} \hrule \vspace{.5em} \large Reaserch Progress Report}

\begin{document}

\pagenumbering{gobble}% Remove page numbers (and reset to 1)

\maketitle

\vspace{5cm}

\begin{abstract}
Who does not want a more powerful and compact chip to process their kinky videos?
The industry has been fulfilling this wish for decades, but struggles to keep it financialy viable.
It is becoming obvious that the classic 2D architectures are being limited by their own technology.
As a famous space ranger once said: “To infinity... and beyond 2D!”
We need to go beyond the 2D paradigms and explore the third dimension.
Numerous research groups are working on different aspects of this vast and problem-prone subject.

We are currently working on automatizing the paritioning of 2D architectures that are still mostly done by hand.
To do so, we are developping a solution interfacing with popular production sofwtares and standards that will process the placed and routed 2D design, extract the underlying hypergraph and statistics, clusterize it, partition it and re-generate netlists for each die.

As of now, we have a hypergraph and statistics extractor as well as a naïve clustering solution for bipartitioning any architecture.
Although existing partitioning libraries were used, new weights metrics have been studied.
The impact of clustering granularity on inter- and intra-cluster connecticity has also been under our scope.

In the near future, the generation of partitioned netlists and multiple dies partitioning will be addressed.
\end{abstract}

\clearpage

\tableofcontents

\clearpage

\pagenumbering{Roman}

% \glsaddall
\printglossaries

\clearpage

\pagenumbering{arabic}% Arabic page numbers (and reset to 1)

\begin{quote}
Imagine you are an antic civil contractor asked to handle high density parking for horse drawn caravans.
You have a fixed area terrain on which you can fit so much cars.
Along the years, they become smaller and less bulkier, but that scaling tend to slow down and you still need to fit more and more of them on your parking.
You now have to make a choice: Do you spend time and money to engineer expensive cars using new materials, or do you finally add a second floor to your parking?
\end{quote}




% ########   ##      ##  ##########  ########     #####    
%    ##      ###     ##      ##      ##     ##  ##     ##  
%    ##      ## ##   ##      ##      ##     ##  ##     ##  
%    ##      ##  ##  ##      ##      ########   ##     ##  
%    ##      ##   ## ##      ##      ##   ##    ##     ##  
%    ##      ##     ###      ##      ##    ##   ##     ##  
% ########   ##      ##      ##      ##     ##    #####    

\section{Motivation: Why do we want to go 3D?}

Stability, security, performance, we always want more.
The same goes for \glspl{ic}.


\subsection{Why is 2D not enough anymore?}
\begin{itemize}
	\item MONEY
	\item Gate capacitance increasing
	\item Manufacturing. 
\end{itemize}






%  #######     #####       ###     
% ##     ##  ##     ##    ## ##    
% ##         ##     ##   ##   ##   
%  #######   ##     ##  ##     ##  
%        ##  ##     ##  #########  
% ##     ##  ##     ##  ##     ##  
%  #######     #####    ##     ## 

\section{State of the Art: How do we go 3D?}
\subsection{Hypergraph partitioning}
\todo[inline]{Define hypergraph here.}
\begin{definition}[Balanced bipartitioning]\label{def:bal-bipart}

\end{definition}

\begin{definition}[Cutsize]\label{def:cutsize}
\end{definition}

\subsubsection{Complexity}

\begin{definition}[Non-deterministic Turing Machine]\label{def:nd-tm}
In this kind of Turing machines, the transition \emph{rule} becomes a transition \emph{function}.
It means that at a given state, with a given symbol, it has many possible execution instead of just the one with a deterministic Turing machine.
We can see that as the machine always taking the best possible guess, leading to a finishing state in the end, or the machine is branching into all the possible states from the point of decision.
\end{definition}

\begin{definition}[NP]\label{def:np}
Acronym for ``non-deterministic polynomial-time".
A ``yes"-instance can be found in polynomial-time by a non-deterministic Turing machine, and this solution can be verified in polynomial-time by a deterministic Turing machine.
\end{definition}

\begin{definition}[NP-hard]\label{def:np-hard}
Acronym for ``non-deterministic polynomial-time hard".
~\newline{}\noindent Note: it has never been proven that there is no polynomial-time algorithm to solve those problems.
Informally, it means that those problems are at least as hard as the hardest problems in NP.
\end{definition}

\begin{definition}[NP Complete]\label{def:np-comp}
A problem is NP complete when it is both NP and NP hard at the same time.
Although the solution can be verified in polynomial time, there is no known algorithm to find this solution quickly.
The problem to know if such algorithm exist is the ``P $=$ NP" problem.

An NP-complete problem differs from a regular NP problem in the sense that a problem is NP-complete if every other problem in NP can be reduced to it.ea
\end{definition}

\begin{definition}[P=NP]\label{def:p-np}
Problem asking if there exist polynomial time algorithm solving NP-complete problems (an by corollary all NP problems).
\end{definition}


\subsection{The Usual Suspects}
A lot of solutions have been developped along the years to solve various partitioning-related problems.
\todo[inline]{Cite some articles like “billion node thing”}
However, most of them are derivations of a few “founders” algorithms.

\subsubsection{Kernighan-Lin Algorithm~\citep*{Kernighan1970}}
\todo[inline]{Describe the mathematical principles of the method.}

\subsubsection{Fiduccia-Mattheyses Algorithm~\citep*{Fiduccia1982}}
\todo[inline]{Describe the mathematical principles of the method.}

\subsubsection{EIG Algorithm~\citep*{Hagen1992}}
\citet{Hagen1992} showed that the second smallest eigenvalue of the connectivity matrix of the graph is a tight lower bound on the ratio cut metric.
That metric is defined as $\frac{c(X,Y)}{|X||Y|}$ with $c(X, Y)$ the cutsize between the two partitions.

Based on this result, they built an algorithm to make bipartition of graphs with minimal ratio cut.

The idea is first to approximate the hypergraph with an undirected graph using a $k$-clique model.
This model states that a clique with k nodes forms a $k$-clique, hence a hyperedge forms one as well.
From this graph, we derive the degree matrix $D$ where  $d_{ii}$ is the sum of all the weights of the incident edges on node $i$.
Similarly, we build the adjacent matrix $A$ where $a_{ij}$ gives the weight of the edge $(i, j)$.
The Laplacian matrix $Q$ is then $Q = D - A$, from which we extract the eigenvector and its second smallest value.
Using the ordered eigenvector, we then compute $n-1$ partitionings: $(\{v_1\}, \{v_2, ..., v_n\}), (\{v_1, v_2\}, \{v_3, ..., v_n\}), ..., (\{v_1, ..., v_{n-1}\}, \{v_n\})$.

In the end, this method is particularly useful to set a lower bound on the ratio cut.


\subsubsection{Multilevel paradigm}
\todo[inline]{What are the fundamental articles for this paradigm?}
\todo[inline]{Karypis has some nice figures.}
\subsubsection{Clustering}
\subsubsection{Implementations}
Numerous software libraries implement some flavor of (hyper)graph partitioning.
\todo[inline]{For each of them, find what sets them appart. Try to group them into categories.}
\begin{itemize}
	\item PaToH (\citet{Aykanat2011})
	\item hMetis (\citet{Karypis1999})
	\item Scotch (\citet{Aykanat2011})
	\item MLPart (\citet{Caldwell2000})
	\item Parkway (\citet{Trifunovic2008})
	\item Zoltan (\citet{Devine2006})
	\item Chaco (\citet{Lotfifar2015})
	\item Jostle (\citet{Walshaw1998})
	\item Party (\citet{Preis97party})
	\item KaFFPa (\citet{Holtgrewe2010})
\end{itemize}
\subsubsection{Why hypergraphs?}
All hypergraphs can be represented as regular graphs, and lots of tools already exists to handle graphs.
When we want to express the wire length between two nodes, although it can easily be done using graph's edge weights, it is not expressable as a hypergraph's hyperedge weight, the hyperedge linking several nodes all connected with different wire lengths.

However, when tackling the partitioning problem, working with hypergraphs gives the vertices belonging to the same hyperedge a coherence.
In the context of 3D ICs, we work with buses connecting blocks of gates, and we want them to be handled as such.

Moreover, \citet{Ihler1993} demonstrated that the mincut partition obtained for a circuit is not as accurate as would be a hypergraph's.
More precisely, they define a \textit{cut-model} in definitions~\ref{def:cut-model} and \ref{def:cut-model-formal}, and show that there is no such thing in general.

\begin{definition}[Cut-model]\label{def:cut-model}
An edge-weighted graph $(V,E)$ is a cut-model for an edge-weighted hypergraph $(V,H)$ if the weight of the edges cut by any bipartition of $V$ in the graph is the same as the weight of the hyperedges cut by the same bipartition in the hypergraph.
\end{definition}

A more formal way to define the principle is as follows~:
\begin{definition}[Cut-model and mincut-model]\label{def:cut-model-formal}
A graph $(V, E)$ on $k$ vertices is a cut-model (for a unit weight hyperedge on $k$ vertices) if the weight of any cut induced by a non-empty proper subset $W$ of $V$ is equal to one.

A graph $(V \cup D,E)$ on $k+d$ vertices is called a min-cut-model (for a unit weight hyperedge on $k$ vertices) if for every non-empty subset $W$ of $V$ we have that the weight of any cut with minimum weight (mincut) under those separating $W$ from $V \setminus W$ is equal to one.
It must be zero fo $W=\emptyset$.%Note: other symbol for empty set: \varnothing
\end{definition}

\subsection{Monolithic vs stacked}
\subsection{Memory-on-logic vs logic-on-logic}




%  #######   ##     ##     ###     ##         ##         #########  ##      ##   #######   #########   #######   
% ##     ##  ##     ##    ## ##    ##         ##         ##         ###     ##  ##         ##         ##     ##  
% ##         ##     ##   ##   ##   ##         ##         ##         ## ##   ##  ##         ##         ##         
% ##         #########  ##     ##  ##         ##         ######     ##  ##  ##  ##   ####  ######      #######   
% ##         ##     ##  #########  ##         ##         ##         ##   ## ##  ##     ##  ##                ##  
% ##     ##  ##     ##  ##     ##  ##         ##         ##         ##     ###  ##     ##  ##         ##     ##  
%  #######   ##     ##  ##     ##  #########  #########  #########  ##      ##  ########   #########   #######  

\section{Challenges: Why is it not done yet?}
\subsection{Heat Dissipation}
\subsection{Manufacturing}
\todo[inline]{Yield for 3D ICs}

\subsection{Optimal Partitioning}

\subsection{Find appropriate metrics}




% ##           ##    #####    ########   ##   ##    
% ##           ##  ##     ##  ##     ##  ##  ##     
% ##           ##  ##     ##  ##     ##  ## ##      
% ##           ##  ##     ##  ########   ####       
%  ##   ###   ##   ##     ##  ##   ##    ##  ##     
%   ## ## ## ##    ##     ##  ##    ##   ##   ##    
%    ###   ###       #####    ##     ##  ##    ##  

\section{Work: Where are we now?}
\todo[inline]{Remind them of the aim: the complete and interfaceable workflow.}

\subsection{Graph Extraction}
\todo[inline]{Explain from what format we can extract a graph.}
We use the outputs from Cadence(?) P\&R to extract the vertices and hyperedges, as well as the weights of the graph.

\subsection{Naïve Clustering}
Our clustering is used to reduce the order of the graph for the partitioner to have an easier job.
The way it is implemented is simplistic and naïve: we slice the die into a grid with each cell being a cluster.
We consider that the \gls{pr} has placed closed to each other standard cells that participate to the same logical function, hence making them a sound logical block that can be used as a cluster.
\todo[inline]{Play on the granularity of the clustering and see the impact on inter-cluster wire length.}

\subsection{Stats extraction}

\subsection{Interface with DEF--PaToH/hMetis--Cadence}




% ########   #########   #######   ##     ##  ##         ##########   #######   
% ##     ##  ##         ##     ##  ##     ##  ##             ##      ##     ##  
% ##     ##  ##         ##         ##     ##  ##             ##      ##         
% ########   ######      #######   ##     ##  ##             ##       #######   
% ##   ##    ##                ##  ##     ##  ##             ##             ##  
% ##    ##   ##         ##     ##  ##     ##  ##             ##      ##     ##  
% ##     ##  #########   #######    #######   #########      ##       #######  

\section{Results: What can we say?}
\subsection{WL}
\subsection{Asymetric partitions}
\subsection{Power aware}





% #########  ##     ##  ##########  ##     ##  ########   #########  
% ##         ##     ##      ##      ##     ##  ##     ##  ##         
% ##         ##     ##      ##      ##     ##  ##     ##  ##         
% ######     ##     ##      ##      ##     ##  ########   ######     
% ##         ##     ##      ##      ##     ##  ##   ##    ##         
% ##         ##     ##      ##      ##     ##  ##    ##   ##         
% ##          #######       ##       #######   ##     ##  #########  

\section{Future}
\subsection{Verilog partitioning}
\subsection{Accessibility/UI}
\subsection{Clustering}
What you need to keep in mind, it's that the multilevel paradigm already clusterizes the vertices of the graph during the coarsening phase.
The purpose of the naïve clustering we are applying on the design, is (1) to reduce the order of the graph before feeding it to the partitioner, and (2) maintain the integrity of logical blocks, hence easing the subsequent \gls{pr} of the partitioned dies.
At the moment, we do not yet need a very efficient clustering for the current purpose.
A future improvement could be to develop a more sophisticated algorithm that could replace the coarsening and uncoarsening of the multilevel paradigm, allowing us to fall back on a raw graph partitioning algorithm.
\subsection{New metrics}
\subsection{Multi-die partitioning}

\todo[inline]{I do not intend to develop a new partitioning method: the aim is not to develop yet another derivation of an existing algo.
I want to focus on a usable worklow interfacable with production tools.}
\clearpage

\newpage
\bibliographystyle{abbrvnat}
% \bibliographystyle{acm}
\bibliography{../thesis/bibliography.bib}

\end{document}
